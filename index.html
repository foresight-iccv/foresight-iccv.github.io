<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting. ICCV 2025.">
  <meta name="keywords" content="ForeSight, Object Detection, Trajectory Forecasting, Streaming Perception, ICCV 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ForeSight</h1>
          <h2 class="subtitle is-3">Multi-View Streaming Joint Object Detection and Trajectory Forecasting</h2>
          <p class="is-size-5">Accepted at <strong>ICCV 2025</strong></p>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://sandropapais.github.io/">Sandro Papais</a>,</span>
            <span class="author-block"><a href="https://letian-wang.github.io/">Letian Wang</a>,</span>
            <span class="author-block"><a href="https://www.trailab.utias.utoronto.ca/brian-cheong">Brian Cheong</a>,</span>
            <span class="author-block"><a href="https://www.trailab.utias.utoronto.ca/steven-waslander">Steven L. Waslander</a></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Toronto</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2508.07089" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2508.07089" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/foresight-iccv/project" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code (coming soon)</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-centered">
          <img src="./static/images/motivation.png" alt="Motivation" style="width: 100%; max-width: 800px;">
          <p class="content has-text-centered" style="margin-top: 1rem; max-width: 700px; margin-left: auto; margin-right: auto;">
            A comparison of temporal learning perception methods related to ForeSight. (a) Traditional object detection methods leverage
            single-frame sensor data. (b) Temporal sparse detectors typically keep memory of past detection locations without explicit forecasting
            capabilities. (c) Typical joint detection-forecasting works pass information from detection to forecasting and rely on tracking for temporal
            information. (d) ForeSight further propagates motion forecast information to future frames to be reused in detection and forecasting.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>We introduce ForeSight, a novel joint detection and forecasting framework for vision-based 3D perception in autonomous vehicles. Traditional approaches treat detection and forecasting as separate sequential tasks, limiting their ability to leverage temporal cues. ForeSight addresses this limitation with a multi-task streaming and bidirectional learning approach, allowing detection and forecasting to share query memory and propagate information seamlessly. The forecast-aware detection transformer enhances spatial reasoning by integrating trajectory predictions from a multiple hypothesis forecast memory queue, while the streaming forecast transformer improves temporal consistency using past forecasts and refined detections. Unlike tracking-based methods, ForeSight eliminates the need for explicit object association, reducing error propagation with a tracking-free model that efficiently scales across multi-frame sequences. Experiments on the nuScenes dataset show that ForeSight achieves state-of-the-art performance, achieving an EPA of 54.9\%, surpassing previous methods by 9.3\%, while also attaining the best mAP  and minADE among multi-view detection and forecasting models.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{papais2025foresight,
  author    = {Papais, Sandro and Wang, Letian and Cheong, Brian and Waslander, Steven L},
  title     = {ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {October},
  year      = {2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://arxiv.org/pdf/2508.07089">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/foresight-iccv/project" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This site was adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies project page template</a>, and is freely available under a
            Creative Commons Attribution-ShareAlike 4.0 International License.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
